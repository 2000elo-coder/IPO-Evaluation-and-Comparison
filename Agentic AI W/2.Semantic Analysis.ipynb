{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Reddit Fetch**\n",
    "\n",
    "We will be fetching Reddit post using official API :)"
   ],
   "id": "4fc61da442914999"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T05:02:25.713269Z",
     "start_time": "2025-11-09T05:02:19.976045Z"
    }
   },
   "source": "!pip install praw\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from praw) (1.9.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.10.5)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:02:58.659843Z",
     "start_time": "2025-11-09T05:02:25.740961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"KHbmxuI59KvPPjZOuPprvA\",\n",
    "    client_secret=\"GWtaNscUFjCWPmGJtg0NIaWVCfXSLw\",\n",
    "    user_agent=\"YOUR_APP_NAME / v1.0\"\n",
    ")\n",
    "\n",
    "keyword = \"Groww IPO\"      # <--- change if needed\n",
    "limit_posts = 50\n",
    "limit_comments = 5\n",
    "\n",
    "records = []\n",
    "\n",
    "for submission in reddit.subreddit(\"all\").search(keyword, limit=limit_posts):\n",
    "\n",
    "    submission.comments.replace_more(limit=0)   # prevents loading \"MoreComments\"\n",
    "\n",
    "    for i, comment in enumerate(submission.comments.list()):\n",
    "        if i >= limit_comments:\n",
    "            break\n",
    "\n",
    "        records.append([\n",
    "            submission.created_utc,\n",
    "            submission.subreddit.display_name,\n",
    "            submission.title,\n",
    "            submission.selftext,\n",
    "            submission.url,\n",
    "            comment.body\n",
    "        ])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    records,\n",
    "    columns=[\"post_date_utc\",\"subreddit\",\"post_title\",\"post_text\",\"post_url\",\"comment_text\"]\n",
    ")\n",
    "\n",
    "df.to_csv(\"reddit_posts_with_comments.csv\", index=False)\n",
    "print(df)\n"
   ],
   "id": "2baa1f4804b3f0ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_date_utc          subreddit  \\\n",
      "0     1.762269e+09       IndianStocks   \n",
      "1     1.762269e+09       IndianStocks   \n",
      "2     1.762269e+09       IndianStocks   \n",
      "3     1.762269e+09       IndianStocks   \n",
      "4     1.762269e+09       IndianStocks   \n",
      "..             ...                ...   \n",
      "229   1.756212e+09  IndianStockMarket   \n",
      "230   1.756212e+09  IndianStockMarket   \n",
      "231   1.761765e+09          IndianIPO   \n",
      "232   1.761765e+09          IndianIPO   \n",
      "233   1.761765e+09          IndianIPO   \n",
      "\n",
      "                                            post_title  \\\n",
      "0        Is it a good idea to invest in the groww ipo?   \n",
      "1        Is it a good idea to invest in the groww ipo?   \n",
      "2        Is it a good idea to invest in the groww ipo?   \n",
      "3        Is it a good idea to invest in the groww ipo?   \n",
      "4        Is it a good idea to invest in the groww ipo?   \n",
      "..                                                 ...   \n",
      "229  Support from SBI and groww to release amount h...   \n",
      "230  Support from SBI and groww to release amount h...   \n",
      "231  New Mainboard IPO - Billionbrains Garage Ventu...   \n",
      "232  New Mainboard IPO - Billionbrains Garage Ventu...   \n",
      "233  New Mainboard IPO - Billionbrains Garage Ventu...   \n",
      "\n",
      "                                             post_text  \\\n",
      "0                                                        \n",
      "1                                                        \n",
      "2                                                        \n",
      "3                                                        \n",
      "4                                                        \n",
      "..                                                 ...   \n",
      "229  https://preview.redd.it/rvl5y2gstclf1.png?widt...   \n",
      "230  https://preview.redd.it/rvl5y2gstclf1.png?widt...   \n",
      "231  Date - 4th Nov to 7th Nov\\n\\nIssue Size - 6632...   \n",
      "232  Date - 4th Nov to 7th Nov\\n\\nIssue Size - 6632...   \n",
      "233  Date - 4th Nov to 7th Nov\\n\\nIssue Size - 6632...   \n",
      "\n",
      "                                              post_url  \\\n",
      "0                 https://i.redd.it/73d423mba9zf1.jpeg   \n",
      "1                 https://i.redd.it/73d423mba9zf1.jpeg   \n",
      "2                 https://i.redd.it/73d423mba9zf1.jpeg   \n",
      "3                 https://i.redd.it/73d423mba9zf1.jpeg   \n",
      "4                 https://i.redd.it/73d423mba9zf1.jpeg   \n",
      "..                                                 ...   \n",
      "229  https://www.reddit.com/r/IndianStockMarket/com...   \n",
      "230  https://www.reddit.com/r/IndianStockMarket/com...   \n",
      "231               https://i.redd.it/1bfdnqljo3yf1.jpeg   \n",
      "232               https://i.redd.it/1bfdnqljo3yf1.jpeg   \n",
      "233               https://i.redd.it/1bfdnqljo3yf1.jpeg   \n",
      "\n",
      "                                          comment_text  \n",
      "0    I love the company. I haven’t looked at the nu...  \n",
      "1                                           Absolutely  \n",
      "2    It is.\\nBut the only thing these are not new s...  \n",
      "3      Absolutely. Just applied for 2100 shares today.  \n",
      "4    Should we be planning to hold long term with g...  \n",
      "..                                                 ...  \n",
      "229  Same is happening with me. I also applied for ...  \n",
      "230  See if they allow unblocking after applying Gr...  \n",
      "231  Where do you get this information from? The de...  \n",
      "232  Guys Lenskart IPO analysis is here [LENSKART I...  \n",
      "233                                      From the RHP.  \n",
      "\n",
      "[234 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Google News",
   "id": "2c843875e48efafb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:27.341039Z",
     "start_time": "2025-11-09T05:03:26.363328Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install feedparser requests beautifulsoup4\n",
   "id": "baa669d4b1020aa0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ashut\\onedrive\\documents\\agenticai\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:31.009865Z",
     "start_time": "2025-11-09T05:03:29.082762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def fetch_google_news(ipo_name: str, limit: int = 20):\n",
    "    query = urllib.parse.quote(ipo_name)\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en-IN&gl=IN&ceid=IN:en\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    results = []\n",
    "    for entry in feed.entries[:limit]:\n",
    "        # summary contains HTML → clean it\n",
    "        soup = BeautifulSoup(entry.summary, \"html.parser\")\n",
    "        clean_summary = soup.get_text(\" \", strip=True)\n",
    "\n",
    "        results.append({\n",
    "            \"title\": entry.title,\n",
    "            \"link\": entry.link,\n",
    "            \"summary\": clean_summary,\n",
    "            \"published\": entry.published\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# example:\n",
    "news = fetch_google_news(\"Groww IPO\", 20)\n",
    "\n",
    "print(f\"fetched {len(news)} articles\")\n",
    "for n in news[:3]:\n",
    "    print(\"----\")\n",
    "    print(n[\"title\"])\n",
    "    print(n[\"summary\"])\n"
   ],
   "id": "82c121cf8512f87c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched 20 articles\n",
      "----\n",
      "Groww IPO day 3: GMP, subscription status, date to review. Apply or not? - livemint.com\n",
      "Groww IPO day 3: GMP, subscription status, date to review. Apply or not? livemint.com Groww IPO Day 3 GMP Live: Check final day subscription status, price band, GMP, allotment date, valuation and more Moneycontrol At close, Groww IPO subscribed 18x The Times of India Groww IPO Day 3: Check Subscription Status & Key Highlights Groww Groww IPO oversubscribed 17.6x; QIB quota sees 22x bids Entrackr Highlights: Groww IPO subscribed over 17 times, but GMP cools to 5% – What’s the big worry? financialexpress.com Groww IPO closes today. Here's what the latest GMP signals India Today Groww IPO GMP Live Updates | GMP slips to 4% on Day 3; bidding ends with total subscription of over 17 ti... The Economic Times Groww IPO gets fully subscribed on Day 2 The Hindu\n",
      "----\n",
      "Lenskart, PhysicsWallah, and Groww: GMP trends suggest up to 22% listing gains for 9 IPOs next week - The Economic Times\n",
      "Lenskart, PhysicsWallah, and Groww: GMP trends suggest up to 22% listing gains for 9 IPOs next week The Economic Times\n",
      "----\n",
      "Groww IPO Day Highlights: Issue booked 17.6x; GMP dips, focus shifts to allotment. 6 steps to check status - livemint.com\n",
      "Groww IPO Day Highlights: Issue booked 17.6x; GMP dips, focus shifts to allotment. 6 steps to check status livemint.com\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agent in Making",
   "id": "c19907bfa76f59f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:38.852015Z",
     "start_time": "2025-11-09T05:03:37.782076Z"
    }
   },
   "cell_type": "code",
   "source": "!pip -q install langchain langchain_groq \"langgraph>=0.2.38\" praw feedparser beautifulsoup4 pandas langchain-community\n",
   "id": "d3264a90e88356d0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:41.613841Z",
     "start_time": "2025-11-09T05:03:40.620086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import praw\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ---- LLM (expects GROQ_API_KEY in env) ----\n",
    "# Set it here if you prefer (or export in your shell):\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "\n",
    "\n",
    "# Utility\n",
    "def utc_iso(ts: float) -> str:\n",
    "    try:\n",
    "        return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ],
   "id": "e9a601c39c734f23",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:48.570573Z",
     "start_time": "2025-11-09T05:03:48.535426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- Reddit Tool ----\n",
    "# Inline credentials (as you requested)\n",
    "REDDIT_CLIENT_ID     = \"KHbmxuI59KvPPjZOuPprvA\"\n",
    "REDDIT_CLIENT_SECRET = \"GWtaNscUFjCWPmGJtg0NIaWVCfXSLw\"\n",
    "REDDIT_USER_AGENT    = \"YOUR_APP_NAME / v1.0\"\n",
    "\n",
    "def _make_reddit() -> praw.Reddit:\n",
    "    return praw.Reddit(\n",
    "        client_id=REDDIT_CLIENT_ID,\n",
    "        client_secret=REDDIT_CLIENT_SECRET,\n",
    "        user_agent=REDDIT_USER_AGENT,\n",
    "    )\n",
    "\n",
    "@tool(\"fetch_reddit_tool\", return_direct=False)\n",
    "def fetch_reddit_tool(keyword: str, limit_posts: int = 50, limit_comments: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch Reddit posts and top-level comments for a keyword across r/all. Returns a list of dicts.\"\"\"\n",
    "    try:\n",
    "        reddit = _make_reddit()\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Reddit auth failed: {e}\"}]\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    try:\n",
    "        for submission in reddit.subreddit(\"all\").search(keyword, limit=limit_posts):\n",
    "            try:\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                # Collect up to limit_comments comments\n",
    "                for i, comment in enumerate(submission.comments.list()):\n",
    "                    if i >= limit_comments:\n",
    "                        break\n",
    "                    records.append({\n",
    "                        \"post_date_utc\": utc_iso(submission.created_utc),\n",
    "                        \"subreddit\": str(submission.subreddit.display_name),\n",
    "                        \"post_title\": submission.title or \"\",\n",
    "                        \"post_text\": submission.selftext or \"\",\n",
    "                        \"post_url\": submission.url or \"\",\n",
    "                        \"comment_text\": getattr(comment, \"body\", \"\") or \"\",\n",
    "                    })\n",
    "            except Exception:\n",
    "                # Even if one submission fails, continue\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Reddit search failed: {e}\"}]\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# ---- Google News Tool ----\n",
    "@tool(\"fetch_google_news_tool\", return_direct=False)\n",
    "def fetch_google_news_tool(ipo_name: str, limit: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch top Google News RSS results for an IPO name. Returns a list of dicts with title, link, summary, published.\"\"\"\n",
    "    import urllib.parse\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    try:\n",
    "        query = urllib.parse.quote(ipo_name)\n",
    "        rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en-IN&gl=IN&ceid=IN:en\"\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        for entry in feed.entries[:limit]:\n",
    "            soup = BeautifulSoup(getattr(entry, \"summary\", \"\"), \"html.parser\")\n",
    "            clean_summary = soup.get_text(\" \", strip=True)\n",
    "            results.append({\n",
    "                \"title\": getattr(entry, \"title\", \"\"),\n",
    "                \"link\": getattr(entry, \"link\", \"\"),\n",
    "                \"summary\": clean_summary,\n",
    "                \"published\": getattr(entry, \"published\", \"\"),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"GNews fetch failed: {e}\"}]\n",
    "    return results\n"
   ],
   "id": "d95f39fc6063bc4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:52.685665Z",
     "start_time": "2025-11-09T05:03:52.669589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    ipo_name: str\n",
    "    reddit: List[Dict[str, Any]]\n",
    "    news: List[Dict[str, Any]]\n",
    "    analysis: str\n",
    "\n",
    "\n",
    "def _truncate_text(text: str, max_chars: int) -> str:\n",
    "    text = text or \"\"\n",
    "    return (text[:max_chars] + \"…\") if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "def fetch_reddit_node(state: AgentState) -> AgentState:\n",
    "    ipo = state[\"ipo_name\"]\n",
    "    # Use IPO keyword as provided + \"IPO\" helper if not present\n",
    "    keyword = ipo if \"ipo\" in ipo.lower() else f\"{ipo} IPO\"\n",
    "    reddit_data = fetch_reddit_tool.func(keyword=keyword, limit_posts=50, limit_comments=5)  # call underlying func\n",
    "    return {**state, \"reddit\": reddit_data}\n",
    "\n",
    "\n",
    "def fetch_news_node(state: AgentState) -> AgentState:\n",
    "    ipo = state[\"ipo_name\"]\n",
    "    news_data = fetch_google_news_tool.func(ipo_name=ipo, limit=20)  # call underlying func\n",
    "    return {**state, \"news\": news_data}\n",
    "\n",
    "\n",
    "def _format_context_snippets(reddit: List[Dict[str, Any]], news: List[Dict[str, Any]], max_reddit: int = 30,\n",
    "                             max_news: int = 12) -> str:\n",
    "    # Reddit snippets\n",
    "    r_lines: List[str] = []\n",
    "    count = 0\n",
    "    for r in reddit:\n",
    "        if \"error\" in r:\n",
    "            continue\n",
    "        if count >= max_reddit:\n",
    "            break\n",
    "        title = _truncate_text(r.get(\"post_title\", \"\"), 160)\n",
    "        ctext = _truncate_text(r.get(\"comment_text\", \"\"), 220)\n",
    "        if not (title or ctext):\n",
    "            continue\n",
    "        r_lines.append(f\"- [r/{r.get('subreddit', '')}] {title} || {ctext}\")\n",
    "        count += 1\n",
    "\n",
    "    # News snippets\n",
    "    n_lines: List[str] = []\n",
    "    count = 0\n",
    "    for n in news:\n",
    "        if \"error\" in n:\n",
    "            continue\n",
    "        if count >= max_news:\n",
    "            break\n",
    "        title = _truncate_text(n.get(\"title\", \"\"), 180)\n",
    "        summ = _truncate_text(n.get(\"summary\", \"\"), 260)\n",
    "        n_lines.append(f\"- {title} :: {summ}\")\n",
    "\n",
    "    ctx = []\n",
    "    if r_lines:\n",
    "        ctx.append(\"### Reddit (sample):\\n\" + \"\\n\".join(r_lines))\n",
    "    if n_lines:\n",
    "        ctx.append(\"### News (sample):\\n\" + \"\\n\".join(n_lines))\n",
    "    return \"\\n\\n\".join(ctx)\n",
    "\n",
    "\n",
    "def analysis_node(state: AgentState) -> AgentState:\n",
    "    ipo = state[\"ipo_name\"]\n",
    "    reddit = state.get(\"reddit\", []) or []\n",
    "    news = state.get(\"news\", []) or []\n",
    "\n",
    "    context_snippets = _format_context_snippets(reddit, news)\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a precise financial sentiment analyst. \"\n",
    "        \"You analyze combined Reddit chatter and news headlines/summaries about a specific IPO. \"\n",
    "        \"You MUST return output ONLY in the exact format:\\n\\n\"\n",
    "        \"Sentiment Score: X.X/5\\n\"\n",
    "        \"Summary:\\n\"\n",
    "        \"- bullet...\\n\"\n",
    "        \"- bullet...\\n\\n\"\n",
    "        \"Guidelines:\\n\"\n",
    "        \"- Score reflects near-term retail + media sentiment (0 = very negative, 5 = very positive).\\n\"\n",
    "        \"- Use short, concrete bullets (max 6) covering demand, risks, valuation vibes, red flags, catalysts.\\n\"\n",
    "        \"- DO NOT include sources, counts, or extra sections. No caveats. No emojis.\\n\"\n",
    "        \"- If data is sparse or mixed, reflect that in the score and bullets.\"\n",
    "    )\n",
    "\n",
    "    user_msg = (\n",
    "        f\"IPO: {ipo}\\n\\n\"\n",
    "        f\"Here are sampled context snippets (Reddit + News). Use them to infer sentiment and produce the required output format.\\n\\n\"\n",
    "        f\"{context_snippets if context_snippets else '[No snippets available]'}\"\n",
    "    )\n",
    "\n",
    "    # LLM call\n",
    "    resp = llm.invoke([(\"system\", system_msg), (\"user\", user_msg)])\n",
    "    analysis_text = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "\n",
    "    return {**state, \"analysis\": analysis_text}\n"
   ],
   "id": "2db83b7b6284d6aa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:56.749092Z",
     "start_time": "2025-11-09T05:03:56.734438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"fetch_reddit\", fetch_reddit_node)\n",
    "workflow.add_node(\"fetch_news\", fetch_news_node)\n",
    "workflow.add_node(\"analyze\", analysis_node)\n",
    "\n",
    "workflow.set_entry_point(\"fetch_reddit\")\n",
    "workflow.add_edge(\"fetch_reddit\", \"fetch_news\")\n",
    "workflow.add_edge(\"fetch_news\", \"analyze\")\n",
    "workflow.add_edge(\"analyze\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ],
   "id": "34dde93a9fe66464",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:03:58.818602Z",
     "start_time": "2025-11-09T05:03:58.802960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_agent(ipo_name: str, save_csv: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs the agent end-to-end and prints the final analysis in the exact requested format.\n",
    "    Optionally saves fetched data to CSVs in the current directory.\n",
    "    \"\"\"\n",
    "    result = app.invoke({\"ipo_name\": ipo_name})\n",
    "\n",
    "    # Optional persistence for debugging\n",
    "    if save_csv:\n",
    "        try:\n",
    "            if result.get(\"reddit\"):\n",
    "                pd.DataFrame(result[\"reddit\"]).to_csv(\"reddit_posts_with_comments.csv\", index=False)\n",
    "            if result.get(\"news\"):\n",
    "                pd.DataFrame(result[\"news\"]).to_csv(\"google_news_results.csv\", index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] CSV save failed: {e}\")\n",
    "\n",
    "    print(result.get(\"analysis\", \"\").strip())\n",
    "    return result\n"
   ],
   "id": "458eaf5f56575770",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:04:31.638417Z",
     "start_time": "2025-11-09T05:04:01.700913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure your GROQ_API_KEY is set before running:\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_KEY\"\n",
    "\n",
    "_ = run_agent(\"Groww IPO\", save_csv=True)\n"
   ],
   "id": "d3787f96bbf2ebf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: 4.2/5\n",
      "Summary:\n",
      "- Strong demand for Groww IPO, subscribed 17.6x on Day 3.\n",
      "- GMP softens to 11% reaching Rs 111, but still signals decent listing.\n",
      "- Positive sentiment from investors, with many applying for shares.\n",
      "- Some concerns about valuation and market conditions, but overall optimism.\n",
      "- Long-term growth potential seen for the company.\n",
      "- Focus shifts to allotment date, with steps to check status available.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
