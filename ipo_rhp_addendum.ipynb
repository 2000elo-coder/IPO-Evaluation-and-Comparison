{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dwftQ1bfvImY",
        "outputId": "bea3dff9-ea0c-45f2-ad18-37ce6ea099f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.10.5 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.10.5)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, rapidfuzz, outcome, trio, bs4, trio-websocket, selenium\n",
            "Successfully installed bs4-0.0.2 outcome-1.3.0.post0 rapidfuzz-3.14.3 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4 selenium pandas rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, unquote\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "\n",
        "\n",
        "class SEBIScraper:\n",
        "    BASE = \"https://www.sebi.gov.in\"\n",
        "    SEARCH_URL = BASE + \"/sebiweb/home/HomeAction.do\"\n",
        "    HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    def __init__(self, folder=\"SEBI_RHPs\", max_retries=3, backoff=1):\n",
        "        self.folder = folder\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        # Create a resilient session\n",
        "        self.session = requests.Session()\n",
        "        retries = Retry(\n",
        "            total=max_retries,\n",
        "            backoff_factor=backoff,\n",
        "            status_forcelist=[500, 502, 503, 504],\n",
        "        )\n",
        "        self.session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    # -------------------------------\n",
        "    # Utility: safe filename\n",
        "    # -------------------------------\n",
        "    def _safe_filename(self, name: str) -> str:\n",
        "        return re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", name)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 1: Search company filings\n",
        "    # -------------------------------\n",
        "    def search_company(self, company_name: str):\n",
        "        params = {\n",
        "            \"doListing\": \"yes\",\n",
        "            \"sid\": \"3\",\n",
        "            \"ssid\": \"15\",\n",
        "            \"smid\": \"11\",\n",
        "            \"search\": company_name,\n",
        "        }\n",
        "        r = self.session.get(self.SEARCH_URL, params=params,\n",
        "                             headers=self.HEADERS, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        results = []\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a[\"href\"]\n",
        "            if \"/filings/public-issues/\" in href:\n",
        "                results.append(urljoin(self.BASE, href))\n",
        "        return list(dict.fromkeys(results))  # deduplicate\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 2: Extract PDFs from filing page\n",
        "    # -------------------------------\n",
        "    def extract_pdfs(self, filing_url: str):\n",
        "        r = self.session.get(filing_url, headers=self.HEADERS, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        pdfs = []\n",
        "\n",
        "        # Case 1: direct <a href=\"...pdf\"> or attachdocs links\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a[\"href\"]\n",
        "            if href.lower().endswith(\".pdf\") or \"attachdocs\" in href or \"file=\" in href:\n",
        "                candidate = urljoin(self.BASE, href)\n",
        "                parsed = urlparse(candidate)\n",
        "                qs = parse_qs(parsed.query)\n",
        "                if \"file\" in qs:  # unwrap real PDF link\n",
        "                    pdfs.append(unquote(qs[\"file\"][0]))\n",
        "                else:\n",
        "                    pdfs.append(candidate)\n",
        "\n",
        "        # Case 2: iframe/embed with ?file=\n",
        "        for tag in soup.find_all([\"iframe\", \"embed\"], src=True):\n",
        "            src = tag[\"src\"]\n",
        "            if \"file=\" in src:\n",
        "                candidate = urljoin(self.BASE, src)\n",
        "                parsed = urlparse(candidate)\n",
        "                qs = parse_qs(parsed.query)\n",
        "                if \"file\" in qs:\n",
        "                    pdfs.append(unquote(qs[\"file\"][0]))\n",
        "                else:\n",
        "                    pdfs.append(candidate)\n",
        "\n",
        "        return list(dict.fromkeys(pdfs))\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 3: Download PDF\n",
        "    # -------------------------------\n",
        "    def download_pdf(self, pdf_url: str, prefix=\"RHP\"):\n",
        "        # Unwrap ?file= link if needed\n",
        "        parsed = urlparse(pdf_url)\n",
        "        qs = parse_qs(parsed.query)\n",
        "        if \"file\" in qs:\n",
        "            pdf_url = unquote(qs[\"file\"][0])\n",
        "\n",
        "        filename = os.path.basename(urlparse(pdf_url).path)\n",
        "        safe_name = self._safe_filename(f\"{prefix}__{filename}\")\n",
        "        path = os.path.join(self.folder, safe_name)\n",
        "\n",
        "        if os.path.exists(path):\n",
        "            print(f\"‚ÑπÔ∏è Already exists: {path}\")\n",
        "            return path\n",
        "\n",
        "        with self.session.get(pdf_url, headers=self.HEADERS, stream=True, timeout=30) as r:\n",
        "            r.raise_for_status()\n",
        "            # Ensure it's a PDF\n",
        "            if \"application/pdf\" not in r.headers.get(\"Content-Type\", \"\"):\n",
        "                print(f\"‚ö†Ô∏è Not a PDF link (may be wrapper): {pdf_url}\")\n",
        "            with open(path, \"wb\") as f:\n",
        "                for chunk in r.iter_content(8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "\n",
        "        print(f\"‚úÖ Saved: {path}\")\n",
        "        return path\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 4: Orchestrator\n",
        "    # -------------------------------\n",
        "    def download_all(self, company_name: str):\n",
        "        filings = self.search_company(company_name)\n",
        "        if not filings:\n",
        "            print(f\"‚ùå No filings found for {company_name}\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Filing pages for {company_name}:\")\n",
        "        for f in filings:\n",
        "            print(\" ‚Ä¢\", f)\n",
        "\n",
        "        results = []\n",
        "        for f in filings:\n",
        "            pdfs = self.extract_pdfs(f)\n",
        "            print(f\"\\nüîó PDFs from {f}:\")\n",
        "            for p in pdfs:\n",
        "                print(\"   \", p)\n",
        "\n",
        "            # Infer filing type from URL (RHP, DRHP, Addendum, Corrigendum)\n",
        "            basename = os.path.basename(urlparse(f).path)\n",
        "            filing_type = (\n",
        "                \"RHP\" if \"rhp\" in basename.lower()\n",
        "                else \"DRHP\" if \"drhp\" in basename.lower()\n",
        "                else \"ADDENDUM\" if \"addendum\" in basename.lower()\n",
        "                else \"CORRIGENDUM\" if \"corrigendum\" in basename.lower()\n",
        "                else \"FILING\"\n",
        "            )\n",
        "\n",
        "            prefix = f\"{company_name.replace(' ', '_')}_{filing_type}\"\n",
        "\n",
        "            for pdf in pdfs:\n",
        "                local = self.download_pdf(pdf, prefix=prefix)\n",
        "                results.append({\n",
        "                    \"company\": company_name,\n",
        "                    \"filing_url\": f,\n",
        "                    \"pdf_url\": pdf,\n",
        "                    \"local_path\": local,\n",
        "                    \"type\": filing_type,\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "yK_Bmdkb0fcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Example usage\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    scraper = SEBIScraper()\n",
        "    company = \"Waaree\"  # try also \"Tata Capital Limited\"\n",
        "    all_files = scraper.download_all(company)\n",
        "    print(\"\\n‚úÖ Completed. Files downloaded:\")\n",
        "    for f in all_files:\n",
        "        print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI3u5vTOvRRj",
        "outputId": "978ad3e3-37a2-4626-b4e0-8aab70d1819f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filing pages for Waaree:\n",
            " ‚Ä¢ https://www.sebi.gov.in/filings/public-issues/oct-2024/waaree-energies-limited-rhp_87562.html\n",
            "\n",
            "üîó PDFs from https://www.sebi.gov.in/filings/public-issues/oct-2024/waaree-energies-limited-rhp_87562.html:\n",
            "    https://www.sebi.gov.in/sebi_data/attachdocs/oct-2024/1728986428790.pdf\n",
            "‚úÖ Saved: SEBI_RHPs/Waaree_RHP__1728986428790.pdf\n",
            "\n",
            "‚úÖ Completed. Files downloaded:\n",
            "{'company': 'Waaree', 'filing_url': 'https://www.sebi.gov.in/filings/public-issues/oct-2024/waaree-energies-limited-rhp_87562.html', 'pdf_url': 'https://www.sebi.gov.in/sebi_data/attachdocs/oct-2024/1728986428790.pdf', 'local_path': 'SEBI_RHPs/Waaree_RHP__1728986428790.pdf', 'type': 'RHP'}\n"
          ]
        }
      ]
    }
  ]
}